{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from recent_neighbor import RecentNeighbor\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import pickle\n",
    "from model import CLTSBR\n",
    "from utils import *\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import Counter\n",
    "# import torch.nn as nn   \n",
    "class Args:\n",
    "    dataset_name = \"diginetica\"\n",
    "    train_dir = \"default\"\n",
    "    batch_size = 2\n",
    "    lr = 0.001\n",
    "    maxlen = 50\n",
    "    hidden_units = 50\n",
    "    num_blocks = 2\n",
    "    num_epochs = 2\n",
    "    num_heads = 1\n",
    "    dropout_rate = 0.5\n",
    "    l2_emb = 0.0\n",
    "    device = \"cpu\"\n",
    "    inference_only = False\n",
    "    state_dict_path = None\n",
    "    num_layers=6    \n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pickle.load(open('data/diginetica/train_session.txt', 'rb'))\n",
    "train_id = train_data[0]\n",
    "train_session = train_data[1]\n",
    "train_timestamp = train_data[2]\n",
    "train_predict = train_data[3]\n",
    "\n",
    "\n",
    "for i, s in enumerate(train_session):\n",
    "    train_session[i] += [train_predict[i]]\n",
    "user_train = {train_data[0][k]: train_data[1][k] for k in range(len(train_data[0]))}\n",
    "usernum, itemnum = 95425, 37522\n",
    "\n",
    "pick_neighbor = RecentNeighbor(session_id=train_id, session=train_session, session_timestamp=train_timestamp, sample_size=0, k=500,\n",
    "             factor1=True, l1=1.25, factor2=True, l2=80 * 24 * 3600, factor3=True, l3=22.5)\n",
    "flat_list = [item for sublist in train_data[1] for item in sublist]\n",
    "cnt = Counter()\n",
    "for num in flat_list:\n",
    "    cnt[num] += 1\n",
    "      \n",
    "most_common = cnt.most_common(7504)\n",
    "head_items = set([num for num, cnt in most_common])\n",
    "total_items = set(flat_list)\n",
    "tail_items = total_items-head_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, user_train, usernum, itemnum, maxlen):\n",
    "        self.user_train = user_train\n",
    "        self.usernum = usernum\n",
    "        self.itemnum = itemnum\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user_train)\n",
    "    \n",
    "    def get_neighbour(self, user, seq):\n",
    "        neighboring_sessions = pick_neighbor.predict(session_id=user, session_items=seq)\n",
    "        return neighboring_sessions\n",
    "        \n",
    "\n",
    "    def __getitem__(self, user_idx):\n",
    "        user = user_idx\n",
    "        seq = np.zeros(self.maxlen, dtype=np.int64)\n",
    "        seq_pop = np.zeros(self.maxlen, dtype=np.float64)\n",
    "        pos = np.zeros(self.maxlen, dtype=np.int64)\n",
    "        neg = np.zeros(self.maxlen, dtype=np.int64)\n",
    "        nxt = user_train[user][-1]\n",
    "        idx = self.maxlen - 1\n",
    "\n",
    "        ts = set(user_train[user])\n",
    "        for i in reversed(user_train[user][:-1]):\n",
    "            seq[idx] = i\n",
    "            seq_pop[idx] = 1/cnt[i]\n",
    "            pos[idx] = nxt\n",
    "            if nxt != 0: neg[idx] = random_neq(1, itemnum + 1, ts)\n",
    "            nxt = i\n",
    "            idx -= 1\n",
    "            if idx == -1: break\n",
    "\n",
    "        n_sess = self.get_neighbour(user, seq)\n",
    "        n_sess = np.array(n_sess).squeeze()\n",
    "        return (user, seq, pos, neg, n_sess, seq_pop)\n",
    "\n",
    "train_dataset = TrainDataset(user_train, usernum, itemnum, args.maxlen)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "batch = next(iter(train_dataloader))\n",
    "print(batch[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLTSBR(usernum, itemnum, args).to(args.device) \n",
    "model.train()\n",
    "\n",
    "epoch_start_idx = 1\n",
    "if args.state_dict_path is not None:\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(args.state_dict_path, map_location=torch.device(args.device)))\n",
    "        tail = args.state_dict_path[args.state_dict_path.find('epoch=') + 6:]\n",
    "        epoch_start_idx = int(tail[:tail.find('.')]) + 1\n",
    "    except: \n",
    "        print('failed loading state_dicts, pls check file path: ', end=\"\")\n",
    "        print(args.state_dict_path)\n",
    "        print('pdb enabled for your quick check, pls type exit() if you do not need it')\n",
    "        \n",
    "\n",
    "if args.inference_only:\n",
    "    model.eval()\n",
    "    t_test = evaluate(model, args.dataset, args, head_items, tail_items)\n",
    "    print('test (NDCG@10: %.4f, HR@10: %.4f)' % (t_test[0], t_test[1]))\n",
    "\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss() # torch.nn.BCELoss()\n",
    "adam_optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, betas=(0.9, 0.98))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 0.0\n",
    "t0 = time.time()\n",
    "\n",
    "for epoch in range(epoch_start_idx, args.num_epochs + 1):\n",
    "    if args.inference_only: break \n",
    "    for batch_idx, (u, seq, pos, neg, n_sess, seq_pop) in enumerate(train_dataloader):\n",
    "        u, seq, pos, neg = np.array(u), np.array(seq), np.array(pos), np.array(neg)\n",
    "        pos_logits, neg_logits = model(u, seq, pos, neg, n_sess.squeeze().type(torch.LongTensor), seq_pop)\n",
    "        pos_labels, neg_labels = torch.ones(pos_logits.shape, device=args.device), torch.zeros(neg_logits.shape, device=args.device)\n",
    "        adam_optimizer.zero_grad()\n",
    "        indices = np.where(pos != 0)\n",
    "        loss = criterion(pos_logits[indices], pos_labels[indices])\n",
    "        loss += criterion(neg_logits[indices], neg_labels[indices])\n",
    "        for param in model.item_emb.parameters(): loss += args.l2_emb * torch.norm(param)\n",
    "        loss.backward()\n",
    "        adam_optimizer.step()\n",
    "        print(\"loss in epoch {} iteration {}: {}\".format(epoch, batch_idx, loss.item())) \n",
    "    if epoch % 10 == 0:\n",
    "        model.eval()\n",
    "        t1 = time.time() - t0\n",
    "        T += t1\n",
    "        print('Evaluating', end='')\n",
    "        t_test = evaluate(model, args.dataset, args, head_items, tail_items)\n",
    "        model.train()\n",
    "\n",
    "    if epoch == args.num_epochs:\n",
    "        folder = args.dataset_name + '_' + args.train_dir\n",
    "        fname = 'CLTSBR.epoch={}.lr={}.layer={}.head={}.hidden={}.maxlen={}.pth'\n",
    "        fname = fname.format(args.num_epochs, args.lr, args.num_blocks, args.num_heads, args.hidden_units, args.maxlen)\n",
    "        torch.save(model.state_dict(), os.path.join(folder, fname))\n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbrec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
